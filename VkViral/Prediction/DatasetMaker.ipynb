{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import cpu_count\n",
    "from nltk.tokenize import word_tokenize\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import vk\n",
    "import urllib.request\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import re\n",
    "import json\n",
    "%matplotlib inline\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = \"9cad73ddf0088749120e2376c2dfcd1a8d89a26b42a3e0e39b3a47833247525e63f4a18b27f41b46d409d\"\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "\n",
    "cores = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_groups():\n",
    "    session = vk.Session(token)\n",
    "    api = vk.API(session)\n",
    "    try:\n",
    "        json_response = api.groups.search(q=\"a\", type=\"page\", count=\"1000\", v=\"5.131\", sort=\"6\")\n",
    "        return json_response['items']\n",
    "    except Exception as ex:\n",
    "        print(ex)\n",
    "\n",
    "def get_mean_reposts(posts):\n",
    "    mean_reposts = 0\n",
    "    for i in range(len(posts)):\n",
    "        mean_reposts += posts[i]['reposts']['count']\n",
    "    mean_reposts = mean_reposts / len(posts)\n",
    "    return mean_reposts\n",
    "\n",
    "def get_posts(group_id, count, offset):\n",
    "    session = vk.Session(token)\n",
    "    api = vk.API(session)\n",
    "    try:\n",
    "        json_response = api.wall.get(owner_id=-group_id, count=count, offset=offset, v=\"5.131\")\n",
    "        posts = json_response['items']\n",
    "        mean_reposts = get_mean_reposts(posts)\n",
    "        for i in range(len(posts)):\n",
    "            posts[i]['virality'] = (posts[i]['reposts']['count'] / mean_reposts) * 1000\n",
    "        return posts\n",
    "    except Exception as ex:\n",
    "        print(ex)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = get_groups()\n",
    "groups = groups[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts = []\n",
    "for i in range(len(groups)):\n",
    "    posts = posts + get_posts(groups[i]['id'], 100, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = pd.DataFrame(posts).to_dict('list')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_photo_average_colour(attachments):\n",
    "    try:\n",
    "        if attachments != None and attachments == attachments:\n",
    "            photos = list(filter(lambda x: x['type']=='photo', attachments))\n",
    "            if any(photos):\n",
    "                url = photos[0]['photo']['sizes'][-1]['url']\n",
    "                urllib.request.urlretrieve(url, \"temp.png\")\n",
    "                img = np.asarray(Image.open('temp.png'))\n",
    "                average_colour_row = np.average(img, axis=0)\n",
    "                average_colour = np.average(average_colour_row, axis=0)\n",
    "                return list(average_colour)\n",
    "        return [0, 0, 0]\n",
    "    except:\n",
    "        return [0, 0, 0]\n",
    "\n",
    "colours = []\n",
    "for i in range(len(dfc['attachments'])):\n",
    "    colours = colours + [first_photo_average_colour(dfc['attachments'][i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_word_count(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^A-Za-zА-Яа-я]\", \" \", text)\n",
    "    text = word_tokenize(text)\n",
    "    return(len(text))\n",
    "\n",
    "word_counts = []\n",
    "for i in range(len(dfc['text'])):\n",
    "    word_counts = word_counts + [get_word_count(dfc['text'][i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_photo_count(attachments):\n",
    "    if attachments != None and attachments == attachments:\n",
    "        photos = list(filter(lambda x: x['type']=='photo', attachments))\n",
    "        return(len(photos))\n",
    "    return 0\n",
    "\n",
    "picture_counts = []\n",
    "for i in range(len(dfc['attachments'])):\n",
    "    picture_counts = picture_counts + [get_photo_count(dfc['attachments'][i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_video_count(attachments):\n",
    "    if attachments != None and attachments == attachments:\n",
    "        videos = list(filter(lambda x: x['type']=='video', attachments))\n",
    "        return(len(videos))\n",
    "    return 0\n",
    "\n",
    "video_counts = []\n",
    "for i in range(len(dfc['attachments'])):\n",
    "    video_counts = video_counts + [get_video_count(dfc['attachments'][i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_audio_count(attachments):\n",
    "    if attachments != None and attachments == attachments:\n",
    "        audios = list(filter(lambda x: x['type']=='audio', attachments))\n",
    "        return(len(audios))\n",
    "    return 0\n",
    "\n",
    "audio_counts = []\n",
    "for i in range(len(dfc['attachments'])):\n",
    "    audio_counts = audio_counts + [get_audio_count(dfc['attachments'][i])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(\"[^A-Za-zА-Яа-я]\", \" \", text)\n",
    "    text = word_tokenize(text)\n",
    "    text = [word for word in text if word not in set(nltk.corpus.stopwords.words(\"russian\"))]\n",
    "    text = [lemmatizer.lemmatize(word) for word in text]\n",
    "    return text\n",
    "\n",
    "texts = []\n",
    "for i in range(len(dfc['text'])):\n",
    "    texts = texts + [preprocess_text(dfc['text'][i])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Запись в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {}\n",
    "data['text'] = texts\n",
    "data['photo'] = picture_counts\n",
    "data['video'] = video_counts\n",
    "data['audio'] = audio_counts\n",
    "data['colour'] = colours\n",
    "data['words'] = list(map(lambda x: len(x), texts))\n",
    "data['likes'] = list(map(lambda x: x['count'], dfc['likes']))\n",
    "data['reposts'] = list(map(lambda x: x['count'], dfc['reposts']))\n",
    "data['views'] = list(map(lambda x: x['count'], dfc['views']))\n",
    "data['comments'] = list(map(lambda x: x['count'], dfc['comments']))\n",
    "data['virality'] = dfc['virality']\n",
    "\n",
    "with open('data.json', 'w') as file:\n",
    "    json.dump(data, file)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
